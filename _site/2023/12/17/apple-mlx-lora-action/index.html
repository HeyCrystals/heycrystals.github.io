<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>基于Apple MLX框架的M1设备上大模型微调实践 &mdash; 八一菜刀</title>
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/collection.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/common.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="http://localhost:4000/2023/12/17/apple-mlx-lora-action/">
    <link rel="alternate" type="application/atom+xml" title="八一菜刀" href="http://localhost:4000/feed.xml">
    <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
    
    <meta property="og:title" content="基于Apple MLX框架的M1设备上大模型微调实践">
      
    <meta name="keywords" content="["Apple MLX框架", "M1设备", "M1设备微调", "大模型微调", "Mistral 7B模型", "训练参数调整", "数据集微调"]">
    <meta name="og:keywords" content="["Apple MLX框架", "M1设备", "M1设备微调", "大模型微调", "Mistral 7B模型", "训练参数调整", "数据集微调"]">
      
    <meta name="description" content="前言">
    <meta name="og:description" content="前言">
      
    
    
        
    
    <meta property="og:url" content="http://localhost:4000/2023/12/17/apple-mlx-lora-action/">
    <meta property="og:site_name" content="八一菜刀">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2023-12-17">
    
    <script src="http://localhost:4000/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="http://localhost:4000/assets/js/jquery-ui.js"></script>
    <script src="http://localhost:4000/assets/js/main.js"></script>
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="http://localhost:4000/" title="八一菜刀"><span class="octicon octicon-mark-github"></span> 八一菜刀</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="http://localhost:4000/" class=" site-header-nav-item" target="" title="首页">首页</a>
                
                <a href="http://localhost:4000/categories/" class=" site-header-nav-item" target="" title="分类">分类</a>
                
                <a href="http://localhost:4000/archives/" class=" site-header-nav-item" target="" title="归档">归档</a>
                
                <a href="http://localhost:4000/about/" class=" site-header-nav-item" target="" title="关于">关于</a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <section class="collection-head small geopattern" data-pattern-id="基于Apple MLX框架的M">
<div class="container">
  <div class="columns">
    <div class="column three-fourths">
      <div class="collection-title">
        <h1 class="collection-header">基于Apple MLX框架的M1设备上大模型微调实践</h1>
        <div class="collection-info">
          
          <span class="meta-info">
            <span class="octicon octicon-calendar"></span> 2023/12/17
          </span>
          
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="http://localhost:4000/categories/#大模型" title="大模型">大模型</a>
          </span>
          
          <span class="meta-info">
            <!--<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>-->
            <span id="busuanzi_container_page_pv"> 浏览<span id="busuanzi_value_page_pv"></span>次</span>
          </span>
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!-- / .banner -->
<section class="container content">
<div class="columns">
  <div class="column three-fourths" >
    <article class="article-content markdown-body">
    <h2 id="前言">前言</h2>

<p>在不久前苹果官方开源发布了针对Apple Silicon 芯片优化的 MLX 深度学习框架，该框架可以简化研究人员在 Mac、iPad、iPhone 平台设计和部署模型的过程。</p>

<p>MLX的主要特性包括：</p>

<ul>
  <li><strong>熟悉的API</strong>：<strong>MLX</strong> 具有紧随 NumPy 的 Python API。 MLX 还拥有功能齐全的 C++ API，它与 Python API 非常相似。 MLX 具有 <code class="language-plaintext highlighter-rouge">mlx.nn</code> 和 <code class="language-plaintext highlighter-rouge">mlx.optimizers</code> 等更高级别的包，其 API 紧密遵循 PyTorch，以简化构建更复杂的模型。</li>
  <li><strong>可组合函数转换</strong>：MLX 具有用于自动微分、自动矢量化和计算图优化的可组合函数转换</li>
  <li><strong>惰性计算 (Lazy computation)</strong>：MLX 中的计算是惰性计算。数组仅在需要时才会具体化</li>
  <li><strong>动态图构建</strong>：MLX 中的计算图采用动态构建，更改函数参数的形状不会触发缓慢的编译，并且调试简单直观</li>
  <li><strong>多设备：</strong>可以在任何支持的设备上运行（当前为 CPU 和 GPU），确保用户能够充分利用硬件</li>
  <li><strong>具备统一内存优势</strong>：MLX 和其他框架的显着区别是采用统一内存模型。 MLX 中的数组位于共享内存中，可以在任何支持的设备类型上执行 MLX 阵列上的操作，而无需移动数据。</li>
</ul>

<p>项目地址：<a href="https://github.com/ml-explore/mlx">https://github.com/ml-explore/mlx</a></p>

<p>而在今天的X上看到Apple开发者分享说可以在32GB的M1设备上使用MLX框架对Mistral 7B(或者llamA)等模型进行微调(Fine-tune)</p>

<p><img src="/assets/images/llm/apple-mlx-lora-action/image-20231216193342777.png" alt="image-20231216193342777" /></p>

<h2 id="准备">准备</h2>

<p>看到官方的例子，我的电脑正好是M1 32GB的配置，就把代码跑来试试看</p>

<p>首先代码下载下来，地址：<a href="https://github.com/ml-explore/mlx-examples/tree/main/lora">https://github.com/ml-explore/mlx-examples/tree/main/lora</a></p>

<p>安装依赖：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p>下载Mistral-7B(14.48GB大小)的模型并解压</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar
<span class="nb">tar</span> <span class="nt">-xf</span> mistral-7B-v0.1.tar
</code></pre></div></div>

<p>将下载下来的模型文件进行转换，执行<code class="language-plaintext highlighter-rouge">convert.py</code>文件, 命令如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 转换命令</span>
python convert.py <span class="se">\</span>
    <span class="nt">--torch-model</span> &lt;path_to_torch_model&gt; <span class="se">\</span>
    <span class="nt">--mlx-model</span> &lt;path_to_mlx_model&gt;
<span class="c"># 转换</span>
python convert.py <span class="se">\</span>
<span class="nt">--torch-model</span> mistral-7B-v0.1 <span class="se">\</span>
<span class="nt">--mlx-model</span> mistral-7b-v0.1-mlx
</code></pre></div></div>

<p>两个主要的参数:</p>

<ul>
  <li>torch-model: Mistral模型的目录，解压后为当前的<code class="language-plaintext highlighter-rouge">mistral-7B-v0.1</code></li>
  <li>mlx-model: 输出目录名称，这里取名<code class="language-plaintext highlighter-rouge">mistral-7b-v0.1-mlx</code></li>
</ul>

<p>通过命令转换后，转换的目录文件会有三个文件，如下图：</p>

<p><img src="/assets/images/llm/apple-mlx-lora-action/image-20231216201202978.png" alt="image-20231216201202978" /></p>

<h2 id="微调fine-tune">微调(Fine-tune)</h2>

<p>将模型下载转换完成后，接下来就可以使用官方提供的<code class="language-plaintext highlighter-rouge">lora.py</code>进行微调(<strong>Fine-tune</strong>)了，先来看数据集：</p>

<p><img src="/assets/images/llm/apple-mlx-lora-action/image-20231216194706972.png" alt="image-20231216194706972" /></p>

<p>训练的数据集是1000行，主要的格式：</p>

<blockquote>
  <p>微调目标是得到一个能够将自然语言句子转换为SQL</p>
</blockquote>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"table: 1-1000181-1</span><span class="se">\n</span><span class="s2">columns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes</span><span class="se">\n</span><span class="s2">Q: Tell me what the notes are for South Australia </span><span class="se">\n</span><span class="s2">A: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = 'SOUTH AUSTRALIA'"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>数据集的格式很清晰：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>table: 表名称
columns: 列名称
Q: 用户问题
A: SQL语句
</code></pre></div></div>

<h2 id="训练">训练</h2>

<p>在第一次train的过程中，直接使用demo中的命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lora.py <span class="nt">--model</span> &lt;path_to_model&gt; <span class="se">\</span>
               <span class="nt">--train</span> <span class="se">\</span>
               <span class="nt">--iters</span> 600
</code></pre></div></div>

<p>运行了大概10分钟后，程序就异常退出了，提示内存不足。</p>

<p><img src="/assets/images/llm/apple-mlx-lora-action/image-20231216195734079.png" alt="image-20231216195734079" /></p>

<p>从图中可以看出，在声明内存的过程中，出现了异常，无法开辟新内存空间，并且每秒的Tokens数量也很感人😭</p>

<p>在看了官方的针对内存的issues建议后，发现有几个参数是影响着内存使用的：</p>

<ul>
  <li><strong>–batch-size</strong>：尝试通过 <code class="language-plaintext highlighter-rouge">--batch-size</code> 使用较小的批量大小。 默认值为 4，因此将其设置为 2 或 1 将减少内存消耗。 这可能会减慢速度，但也会减少内存使用。</li>
  <li><strong>–lora-layers</strong>:少层数以使用 <code class="language-plaintext highlighter-rouge">--lora-layers</code> 进行微调。 默认值为 16，因此您可以尝试 8 或 4。这会减少反向传播所需的内存量。 如果您使用大量数据进行微调，它还可能会降低微调模型的质量。</li>
  <li>数据集：较长的示例需要更多的内存。 如果这对您的数据有意义，您可以做的一件事是在制作 {train, valid, test}.jsonl 文件时将示例分解为更小的序列。</li>
</ul>

<p>根据官方的建议，那么修改train参数，如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lora.py <span class="se">\</span>
   <span class="nt">--model</span> mistral-7b-v0.1-mlx <span class="se">\</span>
   <span class="nt">--train</span> <span class="se">\</span>
   <span class="nt">--batch-size</span> 1 <span class="se">\</span>
   <span class="nt">--lora-layers</span> 4
</code></pre></div></div>

<p>按这个命令执行后，在我的M1设备上执行的还比较快，每秒的Tokens数量平均上110左右</p>

<p><img src="/assets/images/llm/apple-mlx-lora-action/image-20231216200532260.png" alt="image-20231216200532260" /></p>

<p>而Loss的值如下：</p>

<table>
  <thead>
    <tr>
      <th>Iter</th>
      <th>Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>2.265</td>
    </tr>
    <tr>
      <td>200</td>
      <td>1.516</td>
    </tr>
    <tr>
      <td>400</td>
      <td>1.380</td>
    </tr>
    <tr>
      <td>600</td>
      <td>1.350</td>
    </tr>
    <tr>
      <td>800</td>
      <td>1.325</td>
    </tr>
  </tbody>
</table>

<p>train完成后，会在本地默认生成一个权重文件<code class="language-plaintext highlighter-rouge">adapters.npz</code></p>

<p>测试结果：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lora.py <span class="nt">--model</span> mistral-7b-v0.1-mlx <span class="se">\</span>
               <span class="nt">--adapter-file</span> adapters.npz <span class="se">\</span>
               <span class="nt">--num-tokens</span> 50 <span class="se">\</span>
               <span class="nt">--prompt</span> <span class="s2">"table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "</span>
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Generating
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years <span class="k">in </span>Toronto, School/Club Team
Q: What is terrence ross<span class="s1">' nationality
# 大模型输出
A:  SELECT Nationality FROM 1-10015132-16 WHERE Player = '</span>Terrence Ross<span class="s1">' blowing off the rosshill. SELECT Nationality FROM 1-10015
</span></code></pre></div></div>

<p>从结果看，SQL的前半部分写对了，并且也识别出了字段、where条件，但是后面的句子好像就不太对了</p>

<p>我怀疑是在train时，参数<code class="language-plaintext highlighter-rouge">--lora-layers 4</code>的问题，这时，我将改参数改为8，在train一次</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lora.py <span class="se">\</span>
   <span class="nt">--model</span> mistral-7b-v0.1-mlx <span class="se">\</span>
   <span class="nt">--train</span> <span class="se">\</span>
   <span class="nt">--adapter-file</span> adapters_2_8_1.npz <span class="se">\</span>
   <span class="nt">--batch-size</span> 2 <span class="se">\</span>
   <span class="nt">--lora-layers</span> 8
</code></pre></div></div>

<p>而Loss的值如下：</p>

<table>
  <thead>
    <tr>
      <th>Iter</th>
      <th>loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>2.348</td>
    </tr>
    <tr>
      <td>200</td>
      <td>1.392</td>
    </tr>
    <tr>
      <td>400</td>
      <td>1.293</td>
    </tr>
    <tr>
      <td>800</td>
      <td>1.213</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.233</td>
    </tr>
  </tbody>
</table>

<p>之后，同样的命令，再来看效果：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lora.py <span class="nt">--model</span> mistral-7b-v0.1-mlx <span class="se">\</span>
               <span class="nt">--adapter-file</span> adapters_2_8.npz <span class="se">\</span>
               <span class="nt">--num-tokens</span> 50 <span class="se">\</span>
               <span class="nt">--prompt</span> <span class="s2">"table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "</span>
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Generating
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years <span class="k">in </span>Toronto, School/Club Team
Q: What is terrence ross<span class="s1">' nationality
A:  SELECT Nationality FROM 1-10015132-16 WHERE Player = '</span>Terrence Ross<span class="s1">' SELECT Nationality FROM 1-10015132-16 WHERE
</span></code></pre></div></div>

<p>看效果好像在SQL语句中，比上面的效果稍微要好一点了?但是结果还是不对。</p>

<p>效果并没有达到预期，我觉得主要是可能有几个方面的原因：</p>

<ul>
  <li>训练数据集太少，导致大模型可能无法,train.jsonl中的数据集是1000</li>
  <li>参数<code class="language-plaintext highlighter-rouge">--lora-layers </code>的问题，默认是16，虽然我最后改成了8，但是从官方给出的说明来看，该参数会影响质量</li>
</ul>

<p>我将参数<code class="language-plaintext highlighter-rouge">--lora-layers </code>修改为16进行了尝试，跑不了，可能还是我的内存太低了😭，那我只能加数据集了</p>

<p>修改了data目录下的wikisql.py文件，将数据集下载整理的总体数量上升到10000，代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">datanames</span> <span class="o">=</span> <span class="p">[</span><span class="s">"train"</span><span class="p">,</span> <span class="s">"dev"</span><span class="p">,</span> <span class="s">"test"</span><span class="p">]</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">56355</span><span class="p">,</span> <span class="mi">8421</span><span class="p">,</span> <span class="mi">15878</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">dataname</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">datanames</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">WikiSQL</span><span class="p">(</span><span class="n">dataname</span><span class="p">))</span> <span class="o">==</span> <span class="n">size</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong </span><span class="si">{</span><span class="n">dataname</span><span class="si">}</span><span class="s"> set size."</span>

    <span class="c1"># Write the sets to jsonl
</span>    <span class="kn">import</span> <span class="nn">json</span>

    <span class="n">train</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">load</span><span class="p">()</span>
    <span class="c1"># 此处原train参数是1000，我改成5000
</span>    <span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="s">"train"</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span>
        <span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="s">"valid"</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
        <span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="s">"test"</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s">"data/</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">.jsonl"</span><span class="p">,</span> <span class="s">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">dataset</span><span class="p">):</span>
                <span class="c1"># Strip the &lt;s&gt;, &lt;/s&gt; since the tokenizer adds them
</span>                <span class="n">json</span><span class="p">.</span><span class="n">dump</span><span class="p">({</span><span class="s">"text"</span><span class="p">:</span> <span class="n">t</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="o">-</span><span class="mi">4</span><span class="p">]},</span> <span class="n">fid</span><span class="p">)</span>
                <span class="n">fid</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>修改数据集后，在train过后，得到一个新的权重文件，命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lora.py <span class="se">\</span>
   <span class="nt">--model</span> mistral-7b-v0.1-mlx <span class="se">\</span>
   <span class="nt">--train</span> <span class="se">\</span>
   <span class="nt">--adapter-file</span> adapters_2_8_1.npz <span class="se">\</span>
   <span class="nt">--batch-size</span> 2 <span class="se">\</span>
   <span class="nt">--lora-layers</span> 8
</code></pre></div></div>

<p>loss的train过程分值变化：</p>

<table>
  <thead>
    <tr>
      <th>Iter</th>
      <th>loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>2.348</td>
    </tr>
    <tr>
      <td>200</td>
      <td>1.472</td>
    </tr>
    <tr>
      <td>400</td>
      <td>1.410</td>
    </tr>
    <tr>
      <td>600</td>
      <td>1.387</td>
    </tr>
    <tr>
      <td>800</td>
      <td>1.360</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.349</td>
    </tr>
  </tbody>
</table>

<p>再来看看我们的promt得到的结果：</p>

<p><img src="/assets/images/llm/apple-mlx-lora-action/image-20231217105402278.png" alt="image-20231217105402278" /></p>

<p>从结果来看，SQL语句的语法好像并没有什么大的问题，只是结果没有达到预期，可能还是得从数据集及相关参数找一下原因。</p>

<h2 id="结论">结论</h2>

<p>虽然运行的结果还没有完全达到预期，但是在MAC上通过Apple推出的MLX深度学习框架进行Fine-ture的技术方案是可行的。</p>

<p>这也为以后大模型的训练、生态发展提供了另外一种可能性。</p>

<p>包括我们应用开发者在做RAG的过程中，和数据进行对话的场景随着业务的深入肯定会触及，而对模型进行微调是不可避免的。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://github.com/ml-explore/mlx-examples/tree/main/lora">https://github.com/ml-explore/mlx-examples/tree/main/lora</a></li>
  <li><a href="https://github.com/ml-explore/mlx">https://github.com/ml-explore/mlx</a></li>
  <li><a href="https://twitter.com/awnihannun/status/1735782998623261071">https://twitter.com/awnihannun/status/1735782998623261071</a></li>
</ul>

    </article>
    <div class="share">
      <div class="share-component"></div>
    </div>
    <div class="comment">
      

  

  
        <div id="gitalk-container"></div>
        <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
        <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
        <script>
        var gitalk = new Gitalk({
            id: '/2023/12/17/apple-mlx-lora-action/',
            clientID: 'af23e0d8d5f41c7b3719',
            clientSecret: '822f4fcf8e75da6193bd6f26bab3111ecce89142',
            repo: 'blog-comments',
            owner: 'xiaoymin',
            admin: ['xiaoymin'],
            labels: ['gitment'],
            perPage: 50,
        })
        gitalk.render('gitalk-container')
        </script>
  


    </div>
  </div>
  <div class="column one-fourth">
    
<h3>站内搜索</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="搜索">
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="http://localhost:4000/assets/css/modules/sidebar-search.css">
<script src="http://localhost:4000/assets/js/simple-jekyll-search.min.js"></script>
<script src="http://localhost:4000/assets/js/search.js"></script>

<script type="text/javascript">
SimpleJekyllSearch({
    searchInput: document.getElementById('search_box'),
    resultsContainer: document.getElementById('search_results'),
    json: 'http://localhost:4000/assets/search_data.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    limit: 10,
    fuzzy: false,
    exclude: ['Welcome']
})
</script>

    

    <div class="xiaoym-qrcode">
  <div class="xiaoym-qrcode-title">
    <h2>微信公众号(八一菜刀)</h2>
  </div>
  <div style="text-align: center;"><img src="/images/website/mp/qrcode_mini.jpg" /> </div>
</div>

<h3 class="post-directory-title mobile-hidden">Table of Contents</h3>
<div id="post-directory-module" class="mobile-hidden">
  <section class="post-directory">
    <!-- Links that trigger the jumping -->
    <!-- Added by javascript below -->
    <dl></dl>
  </section>
</div>


<script src="http://localhost:4000/assets/js/jquery.toc.js"></script>
  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2015
                    <span title="肖玉民">肖玉民</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="https://github.com/xiaoymin/xiaoymin.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="http://localhost:4000/" title="首页" target="">首页</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/categories/" title="分类" target="">分类</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/archives/" title="归档" target="">归档</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/about/" title="关于" target="">关于</a>
                </li>
                
                <li><a href="http://localhost:4000/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
        <div style="padding-bottom: 40px;
    font-size: 14px;
    height: 14px;
    vertical-align: middle;
    text-align: center;
    color: #999999;
    border-top: 1px solid #eee;">
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <span id="busuanzi_container_site_uv"> <strong>访客数</strong> <span id="busuanzi_value_site_uv"></span></span>
            <span id="busuanzi_container_site_pv"> <strong>访问量</strong> <span id="busuanzi_value_site_pv"></span></span>
        </div>
    </footer>
    <div class="tools-wrapper">
      <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a>
    </div>
    <!-- / footer -->
    <script src="http://localhost:4000/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="http://localhost:4000/assets/js/geopattern.js"></script>
    <script src="http://localhost:4000/assets/js/prism.js"></script>
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>

    

    

    

    

    
</body>
</html>
